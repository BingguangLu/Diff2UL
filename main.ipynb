{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-c1e441d727f3>:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  benign_model.load_state_dict(torch.load(benign_model_path, map_location=device))\n",
      "<ipython-input-5-c1e441d727f3>:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  backdoor_model.load_state_dict(torch.load(backdoor_model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/600000, Loss: 3752.0017\n",
      "Epoch 2000/600000, Loss: 3747.6431\n",
      "Epoch 3000/600000, Loss: 3749.3633\n",
      "Epoch 4000/600000, Loss: 3746.6482\n",
      "Epoch 5000/600000, Loss: 3751.6179\n",
      "Epoch 6000/600000, Loss: 3749.6870\n",
      "Epoch 7000/600000, Loss: 3750.5388\n",
      "Epoch 8000/600000, Loss: 3751.8865\n",
      "Epoch 9000/600000, Loss: 3750.4155\n",
      "Epoch 10000/600000, Loss: 3752.0361\n",
      "Epoch 11000/600000, Loss: 3748.1204\n",
      "Epoch 12000/600000, Loss: 3747.8115\n",
      "Epoch 13000/600000, Loss: 3753.5991\n",
      "Epoch 14000/600000, Loss: 3747.9905\n",
      "Epoch 15000/600000, Loss: 3749.0955\n",
      "Epoch 16000/600000, Loss: 3752.2773\n",
      "Epoch 17000/600000, Loss: 3750.1797\n",
      "Epoch 18000/600000, Loss: 3746.3223\n",
      "Epoch 19000/600000, Loss: 3751.6277\n",
      "Epoch 20000/600000, Loss: 3751.0840\n",
      "Epoch 21000/600000, Loss: 3751.7490\n",
      "Epoch 22000/600000, Loss: 3755.7175\n",
      "Epoch 23000/600000, Loss: 3750.3005\n",
      "Epoch 24000/600000, Loss: 3756.8862\n",
      "Epoch 25000/600000, Loss: 3747.3430\n",
      "Epoch 26000/600000, Loss: 3749.0422\n",
      "Epoch 27000/600000, Loss: 3748.4749\n",
      "Epoch 28000/600000, Loss: 3750.8484\n",
      "Epoch 29000/600000, Loss: 3754.9661\n",
      "Epoch 30000/600000, Loss: 3746.8640\n",
      "Epoch 31000/600000, Loss: 3757.7693\n",
      "Epoch 32000/600000, Loss: 3757.7002\n",
      "Epoch 33000/600000, Loss: 3748.8918\n",
      "Epoch 34000/600000, Loss: 3748.2102\n",
      "Epoch 35000/600000, Loss: 3756.3503\n",
      "Epoch 36000/600000, Loss: 3760.5674\n",
      "Epoch 37000/600000, Loss: 3753.7913\n",
      "Epoch 38000/600000, Loss: 3749.5203\n",
      "Epoch 39000/600000, Loss: 3750.3938\n",
      "Epoch 40000/600000, Loss: 3750.2107\n",
      "Epoch 41000/600000, Loss: 3750.5935\n",
      "Epoch 42000/600000, Loss: 3755.1802\n",
      "Epoch 43000/600000, Loss: 3749.2742\n",
      "Epoch 44000/600000, Loss: 3752.5422\n",
      "Epoch 45000/600000, Loss: 3752.1248\n",
      "Epoch 46000/600000, Loss: 3749.3271\n",
      "Epoch 47000/600000, Loss: 3752.2612\n",
      "Epoch 48000/600000, Loss: 3746.2017\n",
      "Epoch 49000/600000, Loss: 3751.2466\n",
      "Epoch 50000/600000, Loss: 3749.8767\n",
      "Epoch 51000/600000, Loss: 3745.3796\n",
      "Epoch 52000/600000, Loss: 3750.6638\n",
      "Epoch 53000/600000, Loss: 3751.3943\n",
      "Epoch 54000/600000, Loss: 3750.9424\n",
      "Epoch 55000/600000, Loss: 3757.8281\n",
      "Epoch 56000/600000, Loss: 3751.4143\n",
      "Epoch 57000/600000, Loss: 3755.4419\n",
      "Epoch 58000/600000, Loss: 3751.1621\n",
      "Epoch 59000/600000, Loss: 3749.3899\n",
      "Epoch 60000/600000, Loss: 3750.7290\n",
      "Epoch 61000/600000, Loss: 3752.0017\n",
      "Epoch 62000/600000, Loss: 3747.6431\n",
      "Epoch 63000/600000, Loss: 3749.3633\n",
      "Epoch 64000/600000, Loss: 3746.6482\n",
      "Epoch 65000/600000, Loss: 3751.6179\n",
      "Epoch 66000/600000, Loss: 3749.6870\n",
      "Epoch 67000/600000, Loss: 3750.5388\n",
      "Epoch 68000/600000, Loss: 3751.8865\n",
      "Epoch 69000/600000, Loss: 3750.4155\n",
      "Epoch 70000/600000, Loss: 3752.0361\n",
      "Epoch 71000/600000, Loss: 3748.1204\n",
      "Epoch 72000/600000, Loss: 3747.8115\n",
      "Epoch 73000/600000, Loss: 3753.5991\n",
      "Epoch 74000/600000, Loss: 3747.9905\n",
      "Epoch 75000/600000, Loss: 3749.0955\n",
      "Epoch 76000/600000, Loss: 3752.2773\n",
      "Epoch 77000/600000, Loss: 3750.1797\n",
      "Epoch 78000/600000, Loss: 3746.3223\n",
      "Epoch 79000/600000, Loss: 3751.6277\n",
      "Epoch 80000/600000, Loss: 3751.0840\n",
      "Epoch 81000/600000, Loss: 3751.7490\n",
      "Epoch 82000/600000, Loss: 3755.7175\n",
      "Epoch 83000/600000, Loss: 3750.3005\n",
      "Epoch 84000/600000, Loss: 3756.8862\n",
      "Epoch 85000/600000, Loss: 3747.3430\n",
      "Epoch 86000/600000, Loss: 3749.0422\n",
      "Epoch 87000/600000, Loss: 3748.4749\n",
      "Epoch 88000/600000, Loss: 3750.8484\n",
      "Epoch 89000/600000, Loss: 3754.9661\n",
      "Epoch 90000/600000, Loss: 3746.8640\n",
      "Epoch 91000/600000, Loss: 3757.7693\n",
      "Epoch 92000/600000, Loss: 3757.7002\n",
      "Epoch 93000/600000, Loss: 3748.8918\n",
      "Epoch 94000/600000, Loss: 3748.2102\n",
      "Epoch 95000/600000, Loss: 3756.3503\n",
      "Epoch 96000/600000, Loss: 3760.5674\n",
      "Epoch 97000/600000, Loss: 3753.7913\n",
      "Epoch 98000/600000, Loss: 3749.5203\n",
      "Epoch 99000/600000, Loss: 3750.3938\n",
      "Epoch 100000/600000, Loss: 3750.2107\n",
      "Epoch 101000/600000, Loss: 3750.5935\n",
      "Epoch 102000/600000, Loss: 3755.1802\n",
      "Epoch 103000/600000, Loss: 3749.2742\n",
      "Epoch 104000/600000, Loss: 3752.5422\n",
      "Epoch 105000/600000, Loss: 3752.1248\n",
      "Epoch 106000/600000, Loss: 3749.3271\n",
      "Epoch 107000/600000, Loss: 3752.2612\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c1e441d727f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mfine_tune_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# 计算参数差异\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iv/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m                             )\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iv/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iv/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    224\u001b[0m             )\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    227\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iv/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iv/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iv/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    600\u001b[0m                 \u001b[0mexp_avg_sq_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_sqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_max_exp_avg_sqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mexp_avg_sq_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_sqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_exp_avg_sqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_div_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq_sqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义简单的 CNN 模型结构（必须与训练时的模型结构一致）\n",
    "class SimpleCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = torch.nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 加载两个模型\n",
    "benign_model_path = './model/benign_model.pth'\n",
    "backdoor_model_path = './model/backdoor_model.pth'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "benign_model = SimpleCNN().to(device)\n",
    "benign_model.load_state_dict(torch.load(benign_model_path, map_location=device))\n",
    "\n",
    "backdoor_model = SimpleCNN().to(device)\n",
    "backdoor_model.load_state_dict(torch.load(backdoor_model_path, map_location=device))\n",
    "\n",
    "# 创建 60000 个数据点的 X 和随机标签 y\n",
    "num_data_points = 60000\n",
    "X = torch.randn(num_data_points, 1, 28, 28, requires_grad=True, device=device)\n",
    "y = torch.randint(0, 10, (num_data_points,), device=device)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam([X], lr=0.01)\n",
    "\n",
    "# 获取 backdoor 模型参数\n",
    "backdoor_params = {name: param.detach() for name, param in backdoor_model.named_parameters()}\n",
    "\n",
    "# 训练过程\n",
    "num_epochs = 600000  # 必须是 60000 的倍数\n",
    "data_loader = torch.utils.data.DataLoader(torch.arange(num_data_points), batch_size=1, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_id = epoch % num_data_points  # 当前 epoch 对应的数据点索引\n",
    "    idx = epoch_id  # 数据点索引\n",
    "\n",
    "    # Fine-tune benign 模型\n",
    "    fine_tuned_model = SimpleCNN().to(device)\n",
    "    fine_tuned_model.load_state_dict(benign_model.state_dict())\n",
    "    fine_tune_optimizer = torch.optim.Adam(fine_tuned_model.parameters(), lr=0.01)\n",
    "\n",
    "    fine_tuned_model.train()\n",
    "    fine_tune_optimizer.zero_grad()\n",
    "\n",
    "    images = X[idx:idx + 1]  # 取第 idx 个数据点\n",
    "    labels = y[idx:idx + 1]\n",
    "\n",
    "    outputs = fine_tuned_model(images)\n",
    "    loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "    loss.backward()\n",
    "    fine_tune_optimizer.step()\n",
    "\n",
    "    # 计算参数差异\n",
    "    param_diff = 0\n",
    "    for name, param in fine_tuned_model.named_parameters():\n",
    "        param_diff += torch.norm(param - backdoor_params[name])**2\n",
    "\n",
    "    # 优化当前数据点的 X\n",
    "    optimizer.zero_grad()\n",
    "    param_diff.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 打印进度\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {param_diff.item():.4f}\")\n",
    "\n",
    "# 保存优化后的 X\n",
    "optimized_X = X.detach()\n",
    "torch.save(optimized_X, 'optimized_X.pth')\n",
    "print(\"Optimized training set X saved as 'optimized_X.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
